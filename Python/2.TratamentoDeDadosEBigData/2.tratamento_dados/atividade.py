# -*- coding: utf-8 -*-
"""atividade.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13c9VMwpARXWI4aN1RtiYIAm6EF3I5xG_
"""

!pip install pyspark

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, to_date, date_format, count, when

spark = SparkSession.builder.getOrCreate()

# Leia o arquivo ‘videos-stats.csv'  no dataframe 'df_video' com cabeçalho e inferindo o esquema

df_video = spark.read.csv('drive/MyDrive/Spark/dados_atividade/videos-stats.csv', header=True, inferSchema=True)
df_video.show()

# Altere os valores nulos dos campos 'Likes', 'Comments' e 'Views' para o valor 0

df_video = df_video.na.fill({'Likes':0,'Comments':0,'Views':0})

# Leia o arquivo ‘comments.csv' no dataframe 'df_comentario' com cabeçalho e inferindo o esquema

df_comentario = spark.read.csv('drive/MyDrive/Spark/dados_atividade/comments.csv', header=True, inferSchema=True)

df_comentario.show()

# Calcule a quantidade de registros do df_video e df_comentario

print(df_video.count())
print(df_comentario.count())

# Remova os registros do df_video e df_comentario quem possuem o campo 'Video ID' nulos e calcule novamente a quantidade de registros

df_video = df_video.na.drop(subset=['Video ID'])
df_comentario = df_comentario.na.drop(subset=['Video ID'])

print(df_video.count())
print(df_comentario.count())

# Remova os registros apenas do df_video quem possuem o campo 'Video ID' duplicados

df_video = df_video.dropDuplicates(['Video ID'])

df_video.count()

# Converta os campos Likes, Comments e Views para 'long' no dataframe df_video
df_video = df_video.withColumn('Likes', col('Likes').cast('int'))
df_video = df_video.withColumn('Comments', col('Comments').cast('int'))
df_video = df_video.withColumn('Views', col('Views').cast('int'))

df_video.printSchema()

# Converta os campos Likes e Sentiment para 'int' no dataframe df_comentario, além disso, altere o nome do campo Likes para 'Likes Comment'

df_comentario = df_comentario.\
  withColumn('Likes', col('Likes').cast('int')).\
  withColumn('Sentiment', col('Sentiment').cast('int')).\
  withColumnRenamed('Likes', 'Likes Comment')

df_comentario.printSchema()

df_video = df_video.withColumn('Interaction', (col('Likes') + col('Comments') + col('Views')))

df_video.show()

# Converta os campos 'Published At' para 'date' no dataframe df_video

df_video = df_video.withColumn('Published At', to_date(col('Published At')))

df_video.printSchema()

# Crie o campo 'Year' no dataframe df_video, extraindo apenas o ano do campo 'Published At'

df_video = df_video.withColumn('Year', date_format(col('Published At'), 'yyyy'))

df_video.show()

# Mescle os dados df_comentario no dataframe df_video em relação ao campo Video ID e crie o dataframe df_join_video_comments

df_join_video_comments = df_video.join(df_comentario, 'Video ID')

df_join_video_comments.printSchema()

# Leia o arquivo ‘USvideos.csv' no dataframe 'df_us_videos' com cabeçalho e inferindo o esquema

df_us_videos = spark.read.csv('drive/MyDrive/Spark/dados_atividade/USvideos.csv', header=True, inferSchema=True)

df_us_videos.show()

# Mescle os dados df_us_videos no dataframe df_video em relação ao campo Title e crie e visualize o dataframe df_join_video_usvideos

df_join_video_usvideos = df_video.join(df_us_videos, df_video['Title'] == df_us_videos['title'])

df_join_video_usvideos.show()

# Verifique a quantidade de campos nulos em todos os campos do dataframe df_video

valorNulos = 0

for colunas in df_video.columns:
  df_video.select(col(colunas).isNull()) == True
  valorNulos = valorNulos + 1

  print(colunas, valorNulos)

# Remova a coluna '_c0' e salve o dataframe df_video como 'videos-tratados-parquet' no formato parquet e adicione o cabeçalho nos dados

df_video.write.option('header','true').mode("overwrite").parquet('drive/MyDrive/Spark/videos-tratados-parquet')

# Remova a coluna '_c0' e salve o dataframe df_join_video_comments como 'videos-comments-tratados-parquet' no formato parquet e adicione o cabeçalho nos dados

df_join_video_comments.drop('_c0')
df_join_video_comments.write.option('header','true').mode("overwrite").parquet('drive/MyDrive/Spark/videos-comments-tratados-parquet')

